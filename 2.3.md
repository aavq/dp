Отличный кусок документации, и очень правильный шаг — не придумывать порты «по памяти», а вытащить из живых кластеров.

Ниже я дам:

* Набор **безопасных (read-only)** команд `kubectl` и `istioctl` для каждого блока: Ingress, Egress, Control-plane, Envoy admin/metrics, Observability.
* В некоторых местах — небольшой «каркас» скрипта, который можно адаптировать.
* После каждой команды — объяснение **на русском**, что она делает и что именно ты из этого вытащишь в документацию.

Я буду использовать переменные:

```bash
CTX=your-cluster-context     # kube-context кластера
NS_ISTIO=istio-d             # namespace с control-plane
NS_GW=sm-gateways            # namespace с ingress/egress/eastwest gateways
```

Подставь свои значения и добавь `--context="$CTX"` в команды.

---

## 1. Ingress: External LB → istio-ingressgateway

### 1.1. Найти сервисы ingress gateway и их порты

```bash
kubectl --context="$CTX" get svc -n "$NS_GW"
kubectl --context="$CTX" describe svc istio-ingressgateway -n "$NS_GW"
kubectl --context="$CTX" get svc istio-ingressgateway -n "$NS_GW" -o yaml
```

**Что делает:**

* `get svc` — показывает все сервисы в `sm-gateways`, чтобы убедиться, как точно называется ingress.
* `describe svc` — даёт «человеческий» вывод: список портов (`80`, `443`, `15443`, `15021` и т.п.), тип сервиса (`LoadBalancer` / `NodePort`), привязанные `Endpoints`.
* `-o yaml` — даёт структуру в YAML, которую можно сохранить в репо и оттуда уже вытащить:
  `spec.ports[*].port`, `targetPort`, `nodePort`, `type`, `status.loadBalancer.ingress[*].ip/hostname` и т.д.

> Эти команды помогут тебе точно заполнить строку:
> `External client → external LB → istio-ingressgateway (ports 80/443/15443/...)`.

---

### 1.2. Если используется `Ingress`/`Gateway`-контроллер внешнего L4/L7 LB

Если есть `Ingress`-ресурсы (например, внешний nginx/F5/ingress-nginx):

```bash
kubectl --context="$CTX" get ingress -A
kubectl --context="$CTX" describe ingress -A
```

**Что делает:**

* Показывает, есть ли классические `Ingress` c `ingressClassName`, hostname’ами и аннотациями для внешнего L7 LB.
* Часто через них тоже можно понять домены, к которым привязан `istio-ingressgateway`, либо — что часть трафика идёт мимо Istio.

---

### 1.3. Istio Gateway/VirtualService для Ingress

```bash
# Все Gateway (классические Istio ресурсы, не Gateway API)
kubectl --context="$CTX" get gateway -A

# С деталями по конкретному Gateway, связанному с ingressgateway
kubectl --context="$CTX" get gateway -A -o yaml \
  > cluster-$CTX-istio-gateways.yaml

# Все VirtualService
kubectl --context="$CTX" get virtualservice -A -o yaml \
  > cluster-$CTX-istio-virtualservices.yaml
```

**Что делает:**

* `Gateway` — увидишь `spec.servers.port.number` (`80`, `443`, `15443`) и `tls.mode` (`PASSTHROUGH`, `SIMPLE`, `MUTUAL` и т.д.). Это поможет точно описать, какие порты на Envoy слушаются и с каким TLS-режимом.
* `VirtualService` — показывает, какие host’ы и пути реквестов мапятся на какие сервисы/подсети.
  Отсюда можно вывести записи вида:
  `istio-ingressgateway → service-a sidecar → 8080/TCP → mTLS`.

---

### 1.4. Проверить реальные листенеры на ingress gateway (Envoy)

```bash
# Найти Pod ingress gateway
kubectl --context="$CTX" get pods -n "$NS_GW" | grep istio-ingressgateway

POD_IGW=<имя-pod>

# Посмотреть список слушающих портов в Envoy
istioctl --context="$CTX" proxy-config listeners "$POD_IGW" -n "$NS_GW"
```

**Что делает:**

* `istioctl proxy-config listeners` запрашивает у Envoy реальную конфигурацию: какие IP/порты/listeners открыты (`0.0.0.0:80`, `0.0.0.0:443`, `0.0.0.0:15443`, `127.0.0.1:15021` и т.д.).
* Это **самый точный источник** того, какие порты реально слушает ingress, уже после применения всех CRD.

> Очень удобно: можно копировать этот вывод в документацию (или парсить) и показать фактический список портов ingress gateway.

---

## 2. Ingress: istio-ingressgateway → service sidecars (внутри mesh)

### 2.1. Маршруты и кластеры (VirtualService / DestinationRule)

Для конкретного сервиса/неймспейса, куда приходит трафик:

```bash
# Список маршрутов (HTTP/HTTPS) для Pod'а Service A
POD_SA=<pod-service-a>
NS_APP=<namespace-service-a>

istioctl --context="$CTX" proxy-config routes "$POD_SA" -n "$NS_APP"
```

**Что делает:**

* Показывает, какие HTTP/HTTPS маршруты видит Envoy sidecar: какие host’ы/пути → какие `cluster`’ы.
* Этим можно проверить, что VirtualService действительно направляет трафик туда, куда ты ожидаешь.

Сами объекты:

```bash
# DestinationRule по всему кластеру
kubectl --context="$CTX" get destinationrule -A -o yaml \
  > cluster-$CTX-istio-destinationrules.yaml
```

**Что делает:**

* `DestinationRule` определяет subsets, TLS-политику и прочие настройки для upstream’ов. Отсюда можно понять, где включён `ISTIO_MUTUAL` (мэш мTLS), где `DISABLE` и т.п.

---

## 3. Egress: service sidecar → istio-egressgateway → внешние системы

### 3.1. Порты и тип сервиса egress gateway

```bash
kubectl --context="$CTX" get svc -n "$NS_GW" | grep egress
kubectl --context="$CTX" describe svc istio-egressgateway -n "$NS_GW"
kubectl --context="$CTX" get svc istio-egressgateway -n "$NS_GW" -o yaml
```

**Что делает:**

* Даёт список портов egress gateway, его тип (`ClusterIP` или `LoadBalancer`), а также `targetPort`.
* В итоге ты можешь точно написать:
  `service sidecar → istio-egressgateway :443/TCP (mTLS внутри mesh) → внешние SaaS/DB/API`.

---

### 3.2. ServiceEntry, VirtualService для egress

```bash
# Все ServiceEntry
kubectl --context="$CTX" get serviceentry -A -o yaml \
  > cluster-$CTX-istio-serviceentries.yaml

# VirtualService, которые ссылаются на egressgateway
kubectl --context="$CTX" get virtualservice -A -o yaml \
  | yq 'select(.spec.http[].route[].destination.host == "istio-egressgateway.<ns>.svc.cluster.local")' \
  > cluster-$CTX-istio-egress-vs.yaml
```

> Если `yq` не установлен или вы под комплаенсом — можно сначала просто сохранить `virtualservices` в YAML, а фильтрацию сделать локально/офлайн.

**Что делает:**

* `ServiceEntry` — список внешних хостов и портов, которые официально описаны в Istio. Именно оттуда ты берёшь список внешних направлений (`example.com:443`, `db.partner:5432` и т.д.) для документации.
* `VirtualService` с egressgateway — показывает, кто и как маршрутизирует трафик через egress gateway.

---

### 3.3. Листенеры egress Envoy

```bash
POD_EGW=$(kubectl --context="$CTX" get pods -n "$NS_GW" | grep istio-egressgateway | awk '{print $1}')

istioctl --context="$CTX" proxy-config listeners "$POD_EGW" -n "$NS_GW"
```

**Что делает:**

* Показывает, какие порты реально слушает egress Envoy (например, `0.0.0.0:443`, `0.0.0.0:15443`, `127.0.0.1:15021`).
* Можно убедиться, что мTLS включён (`ISTIO_MUTUAL` в clusters) и какие upstream’ы доступны.

---

## 4. Control-plane: istiod ↔ Envoy (xDS, health, metrics)

### 4.1. Сервис istiod и его порты

```bash
kubectl --context="$CTX" get svc -n "$NS_ISTIO"
kubectl --context="$CTX" describe svc istiod -n "$NS_ISTIO"
kubectl --context="$CTX" get svc istiod -n "$NS_ISTIO" -o yaml
```

**Что делает:**

* Показывает, какие порты опубликованы у `istiod` (`15010`, `15012`, `15014` и т.п.), какие типы (`ClusterIP`) и какие `selector`’ы.
* Из этого можно прямо взять строки:

  * `xDS: 15012/TCP (mTLS)`
  * `metrics: 15014/TCP (HTTP)`.

---

### 4.2. Как Envoy видит контрольную плоскость

Для любого sidecar:

```bash
POD_APP=<под с сайдкаром>
NS_APP=<его неймспейс>

istioctl --context="$CTX" proxy-config bootstrap "$POD_APP" -n "$NS_APP" \
  > cluster-$CTX-$POD_APP-bootstrap.json
```

**Что делает:**

* Выгружает bootstrap-конфиг Envoy в JSON:
  там видно:

  * `adminPort` (обычно `15000`),
  * адрес xDS-сервера (`istiod.istio-d.svc:15012`),
  * параметры TLS и т.д.
* Этот файл — отличный артефакт для документации раздела «Control-plane ports & addresses».

---

## 5. Envoy admin & metrics: 15000, 15090 и т.п.

### 5.1. Список листенеров, админ-порт и метрики на sidecar’е

```bash
POD_APP=<под с сайдкаром>
NS_APP=<его неймспейс>

# Листенеры (порты, на которых слушает Envoy sidecar)
istioctl --context="$CTX" proxy-config listeners "$POD_APP" -n "$NS_APP"

# Кластеры (upstreams)
istioctl --context="$CTX" proxy-config clusters "$POD_APP" -n "$NS_APP"

# Эндпоинты (реальные IP:port, куда ходит Envoy)
istioctl --context="$CTX" proxy-config endpoints "$POD_APP" -n "$NS_APP"
```

**Что делает:**

* `listeners` — увидишь все порты, через которые Envoy принимает трафик внутри Pod’а:
  например `0.0.0.0:15090` (metrics), `127.0.0.1:15000` (admin), `0.0.0.0:15006` (inbound/outbound capture), etc.
* `clusters` и `endpoints` — дают полную картину того, куда реально пойдёт трафик (имена сервисов → IP:порт).

При необходимости посмотреть admin UI локально (но это уже скорее для отладки, а не для документации):

```bash
kubectl --context="$CTX" port-forward pod/"$POD_APP" -n "$NS_APP" 15000:15000
# затем в браузере: http://127.0.0.1:15000
```

> Это *read-only* для Envoy (пока не включены dangerous-команды), но с точки зрения комплаенса может считаться «debug only». Для документации достаточно `proxy-config`.

---

## 6. Observability: Kiali, tracing, Prometheus

### 6.1. Найти Kiali и его сервисы/порты

```bash
kubectl --context="$CTX" get pods -A | grep -i kiali
kubectl --context="$CTX" get svc -A | grep -i kiali
```

После того как увидишь namespace и имя:

```bash
NS_KIALI=<namespace Kiali>
kubectl --context="$CTX" describe svc -n "$NS_KIALI" kiali
kubectl --context="$CTX" get svc -n "$NS_KIALI" kiali -o yaml \
  > cluster-$CTX-kiali-svc.yaml
```

**Что делает:**

* Даст тебе точные порты (по умолчанию 20001/TCP, но у вас может быть иначе).
* Из `yaml` легко вытащить `port`, `targetPort`, `type` (ClusterIP/NodePort/LoadBalancer) и написать:
  `Kiali: port 20001/TCP, internal only, exposed via ….domain`.

---

### 6.2. Tracing (Jaeger/Tempo/Zipkin)

```bash
kubectl --context="$CTX" get svc -A | egrep -i 'jaeger|tempo|zipkin'
```

Далее по конкретному сервису:

```bash
NS_TRACE=<namespace>
SVC_TRACE=<service-name>

kubectl --context="$CTX" describe svc -n "$NS_TRACE" "$SVC_TRACE"
kubectl --context="$CTX" get svc -n "$NS_TRACE" "$SVC_TRACE" -o yaml \
  > cluster-$CTX-tracing-svc.yaml
```

**Что делает:**

* Показывает порты UI (`16686` для Jaeger), gRPC/HTTP-ingest портов, тип сервиса — всё, что нужно для секции:
  `Tracing: port 16686/TCP (UI), internal only, + ingest ports XXX`.

---

### 6.3. Prometheus (scrape Envoy/istiod metrics)

```bash
kubectl --context="$CTX" get svc -A | grep -i prometheus
```

Если используете Prometheus Operator и `ServiceMonitor`:

```bash
kubectl --context="$CTX" get servicemonitor -A \
  | egrep -i 'istio|envoy|istiod|istio-ingressgateway|istio-egressgateway'
kubectl --context="$CTX" get servicemonitor -A -o yaml \
  > cluster-$CTX-servicemonitors-istio.yaml
```

**Что делает:**

* Показывает, какие сервисы и порты скрапятся (обычно `15090` на sidecar/gateway, `15014` на istiod).
* Это можно прям табличкой внести в документацию:
  `Prometheus → istiod: 15014/TCP`, `Prometheus → ingressgateway: 15090/TCP`.

---

## 7. Полуавтомат: собрать всё в одну таблицу (опционально)

Простейший скрипт, который собирает только **сервисы Istio** и **их порты** в CSV (для дальнейшей ручной доработки под формат `Source → Destination → Port/Protocol`):

```bash
#!/usr/bin/env bash
set -euo pipefail

CTX="$1"
NS_ISTIO=istio-d
NS_GW=sm-gateways

echo "source,destination,port,protocol,direction"

# istio-ingressgateway service ports
kubectl --context="$CTX" get svc istio-ingressgateway -n "$NS_GW" -o json \
  | jq -r '.spec.ports[] | "external-client,istio-ingressgateway,\(.port),TCP,ingress"'

# istio-egressgateway service ports
kubectl --context="$CTX" get svc istio-egressgateway -n "$NS_GW" -o json \
  | jq -r '.spec.ports[] | "mesh-workloads,istio-egressgateway,\(.port),TCP,internal"'

# istiod service ports
kubectl --context="$CTX" get svc istiod -n "$NS_ISTIO" -o json \
  | jq -r '.spec.ports[] | "envoy-proxies,istiod,\(.port),TCP,control-plane"'
```

**Что делает:**

* Ничего не меняет в кластере, только читает `Service` и печатает CSV, которое можно:

  * загрузить в Excel,
  * вручную дополнить столбцами `Encryption`, `Authentication`, `NetworkPolicy rule`,
  * скопировать в Markdown таблицу.

> Если `jq` под комплаенсом нельзя, можно просто сохранять `-o yaml` и делать постобработку локально или даже руками.

---

Если хочешь, могу следующим шагом помочь тебе на основе **реального вывода** с одного кластера собрать `Data flows & ports` уже в виде готовой таблички для документации (без чувствительных данных — например, ты можешь подставить «обезличенный» вывод или слегка отредактировать IP/hostnames).
